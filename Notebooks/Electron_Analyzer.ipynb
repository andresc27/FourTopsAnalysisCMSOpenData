{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b91d8f18",
   "metadata": {},
   "source": [
    "# Notebook - Electron Analyzer\n",
    "\n",
    "This notebook takes CMS OpenData nanoAOD files, applies some selection and make few simple plots. \n",
    "\n",
    "Let's first load the libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76764285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "\n",
    "import vector; vector.register_awkward()\n",
    "import awkward as ak\n",
    "from coffea import processor\n",
    "from coffea.nanoevents import transforms\n",
    "from coffea.nanoevents.methods import base, vector\n",
    "from coffea.nanoevents import NanoEventsFactory, NanoAODSchema\n",
    "import hist\n",
    "import json\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import uproot\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56108f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/cvmfs/sft.cern.ch/lcg/views/LCG_103swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:193: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx1 => SubJet\n",
      "  warnings.warn(\n",
      "/cvmfs/sft.cern.ch/lcg/views/LCG_103swan/x86_64-centos7-gcc11-opt/lib/python3.9/site-packages/coffea/nanoevents/schemas/nanoaod.py:193: RuntimeWarning: Missing cross-reference index for FatJet_subJetIdx2 => SubJet\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#ifile = uproot.open(\"root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/opendata_files/SingleElectron/cmsopendata2015_Run2015D_SingleElectron_MINIAOD_08Jun2016-v1_21.root\")\n",
    "#ifile[\"Events\"].keys()\n",
    "\n",
    "events = NanoEventsFactory.from_root(\n",
    "    #\"https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root\",\n",
    "    uproot.open(\"root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/opendata_files/SingleElectron/cmsopendata2015_Run2015D_SingleElectron_MINIAOD_08Jun2016-v1_21.root\"),\n",
    "    schemaclass=NanoAODSchema.v6,\n",
    "    metadata={\"dataset\": \"TT\"},\n",
    ").events()\n",
    "#print(ak.count(events.Jet.pt,axis=1)>=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89775383",
   "metadata": {},
   "source": [
    "For future use, let's define some global configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a6c4cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GLOBAL CONFIGURATION\n",
    "\n",
    "DATA = \"SingleElectron\"  ### or SingleElectron\n",
    "\n",
    "# input files per process, set to e.g. 10 (smaller number = faster)\n",
    "N_FILES_MAX_PER_SAMPLE = 10\n",
    "\n",
    "### BENCHMARKING-SPECIFIC SETTINGS\n",
    "\n",
    "# chunk size to use\n",
    "CHUNKSIZE = 500_000\n",
    "\n",
    "# metadata to propagate through to metrics\n",
    "CORES_PER_WORKER = 2  # does not do anything, only used for metric gathering (set to 2 for distributed coffea-casa)\n",
    "\n",
    "# scaling for local setups with FuturesExecutor\n",
    "NUM_CORES = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7cb6b1",
   "metadata": {},
   "source": [
    "NanoAOD datasets are stored in `data/ntuples_nanoaod.json` folder. This json file contains information about the number of events, process and systematic. The following function reads the json file and returns a dictionary with the process to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58a76cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_fileset(n_files_max_per_sample,\n",
    "                      dataset=\"SingleElectron\",\n",
    "                      onlyNominal=False,\n",
    "                      ntuples_json=\"ntuples_nanoaod_V2.json\"):\n",
    "    # using https://atlas-groupdata.web.cern.ch/atlas-groupdata/dev/AnalysisTop/TopDataPreparation/XSection-MC15-13TeV.data\n",
    "    # for reference\n",
    "    # x-secs are in pb\n",
    "    xsec_info = {\n",
    "        \"ttbar\": 831.76, ###396.87 + 332.97, # nonallhad + allhad, keep same x-sec for all\n",
    "        #\"single_top_s_chan\": 2.0268 + 1.2676,\n",
    "        #\"single_top_t_chan\": (36.993 + 22.175)/0.252,  # scale from lepton filter to inclusive\n",
    "        \"single_top_tW\": 35.6 + 35.6, #37.936 + 37.906,\n",
    "        \"wjets\": 61526.7, ##61457 * 0.252,  # e/mu+nu final states\n",
    "        \"tttt\" : 0.009, \n",
    "        \"data\": None\n",
    "    }\n",
    "\n",
    "    # list of files\n",
    "    with open(ntuples_json) as f:\n",
    "        file_info = json.load(f)\n",
    "\n",
    "    # process into \"fileset\" summarizing all info\n",
    "    fileset = {}\n",
    "    for process in file_info.keys():\n",
    "        if process == \"data\":\n",
    "            file_list = file_info[process][dataset][\"files\"]\n",
    "            if n_files_max_per_sample != -1:\n",
    "                #file_list = file_list[:int(n_files_max_per_sample/10)]  # use partial set of samples\n",
    "                file_list = file_list[:]  # use partial set of samples\n",
    "\n",
    "            file_paths = [f[\"path\"] for f in file_list]\n",
    "            metadata = {\"process\": \"data\", \"xsec\": 1}\n",
    "            fileset.update({\"data\": {\"files\": file_paths, \"metadata\": metadata}})\n",
    "            \n",
    "\n",
    "        for variation in file_info[process].keys():\n",
    "            if onlyNominal & ~variation.startswith(\"nominal\"): continue\n",
    "            #print(variation)\n",
    "            file_list = file_info[process][variation][\"files\"]\n",
    "            if n_files_max_per_sample != -1:\n",
    "                file_list = file_list[:n_files_max_per_sample]  # use partial set of samples\n",
    "\n",
    "            file_paths = [f[\"path\"] for f in file_list]\n",
    "            nevts_total = sum([f[\"nevts\"] for f in file_list])\n",
    "            metadata = {\"process\": process, \"variation\": variation, \"nevts\": nevts_total, \"xsec\": xsec_info[process]}\n",
    "            fileset.update({f\"{process}__{variation}\": {\"files\": file_paths, \"metadata\": metadata}})\n",
    "\n",
    "    return fileset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6873da60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processes in fileset: ['ttbar__nominal', 'single_top_tW__nominal', 'wjets__nominal', 'tttt__nominal', 'data']\n",
      "\n",
      "example of information in fileset:\n",
      "{\n",
      "  'files': [https://xrootd-local.unl.edu:1094//store/user/AGC/nanoAOD/TT_TuneCUETP8M1_13TeV-powheg-pythia8/cmsopendata2015_ttbar_19980_PU25nsData2015v1_76X_mcRun2_asymptotic_v12_ext3-v1_00000_0000.root, ...],\n",
      "  'metadata': {'process': 'ttbar', 'variation': 'nominal', 'nevts': 11378043, 'xsec': 831.76}\n",
      "}\n",
      "\n",
      "example of data information in fileset:\n",
      "{\n",
      "  'files': [root://eosuser.cern.ch//eos/user/a/algomez/tmpFiles/opendata_files/SingleElectron/cmsopendata2015_Run2015D_SingleElectron_MINIAOD_08Jun2016-v1_00.root, ...],\n"
     ]
    }
   ],
   "source": [
    "fileset = construct_fileset(N_FILES_MAX_PER_SAMPLE, dataset=DATA,\n",
    "                            onlyNominal=True, ntuples_json='../data/ntuples_nanoaod_V2.json') \n",
    "\n",
    "print(f\"processes in fileset: {list(fileset.keys())}\")\n",
    "print(f\"\\nexample of information in fileset:\\n{{\\n  'files': [{fileset['ttbar__nominal']['files'][0]}, ...],\")\n",
    "print(f\"  'metadata': {fileset['ttbar__nominal']['metadata']}\\n}}\")\n",
    "print(f\"\\nexample of data information in fileset:\\n{{\\n  'files': [{fileset['data']['files'][0]}, ...],\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ed54bc2",
   "metadata": {},
   "source": [
    "## Analyzer\n",
    "\n",
    "Here is the main analyzer. Uses coffea/awkward to make the analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c01ffaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class fourTopAnalysis(processor.ProcessorABC):\n",
    "    def __init__(self, DATASET):\n",
    "        \n",
    "        self.DATASET = DATASET\n",
    "        \n",
    "        #signal i.e. tttt\n",
    "        self.n_jets_sig = []\n",
    "        self.n_bjets_sig = []\n",
    "        self.ht_sig = []\n",
    "        self.htb_sig = []\n",
    "        self.htratio_sig = []\n",
    "        \n",
    "        #tt\n",
    "        self.n_jets_tt = []\n",
    "        self.n_bjets_tt = []\n",
    "        self.ht_tt = []\n",
    "        self.htb_tt = []\n",
    "        self.htratio_tt = []\n",
    "        \n",
    "        #background\n",
    "        self.n_jets_bkg = []\n",
    "        self.n_bjets_bkg = []\n",
    "        self.ht_bkg = []\n",
    "        self.htb_bkg = []\n",
    "        self.htratio_bkg = []\n",
    "        \n",
    "        #data\n",
    "        self.n_jets_data = []\n",
    "        self.n_bjets_data = []\n",
    "        self.ht_data = []\n",
    "        self.htb_data = []\n",
    "        self.htratio_data = []\n",
    "       \n",
    "        #### booking histograms\n",
    "        ## define categories\n",
    "        process_cat = hist.axis.StrCategory([], name=\"process\", label=\"Process\", growth=True)\n",
    "        variation_cat  = hist.axis.StrCategory([], name=\"variation\", label=\"Systematic variation\", growth=True)\n",
    "        \n",
    "        ## define bins (axis)\n",
    "        pt_axis = hist.axis.Regular( bins=500, start=0, stop=500, name=\"var\")\n",
    "        eta_axis = hist.axis.Regular( bins=40, start=-5, stop=5, name=\"var\")\n",
    "        num_axis = hist.axis.Regular( bins=40, start=1, stop=16 , name=\"var\")\n",
    "        num_bjets_axis = hist.axis.Regular( bins=6, start=1.5, stop=7.5 , name=\"var\")\n",
    "        \n",
    "        ## define new variable bins\n",
    "        HTb_axis = hist.axis.Regular( bins=200, start=50, stop=1000, name=\"var\")\n",
    "        HT_axis = hist.axis.Regular( bins=200, start=200, stop=2500, name=\"var\")\n",
    "        HTratio_axis = hist.axis.Regular( bins=500, start=0, stop=1, name=\"var\")\n",
    "        \n",
    "        ## define a dictionary of histograms\n",
    "        self.hist_muon_dict = {\n",
    "            'ele_pt'   : (hist.Hist(pt_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'ele_eta'  : (hist.Hist(eta_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'neles'    : (hist.Hist(num_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'jets_pt'  : (hist.Hist(pt_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'jets_eta' : (hist.Hist(eta_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'njets'    : (hist.Hist(num_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'nbjets'   : (hist.Hist(num_bjets_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'sumbjets_pt'   : (hist.Hist(HTb_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'H_pt'   : (hist.Hist(HT_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "            'Hratio_pt'   : (hist.Hist(HTratio_axis, process_cat, variation_cat, storage=hist.storage.Weight())),\n",
    "\n",
    "        }\n",
    "         \n",
    "        sumw_dict = {'sumw': processor.defaultdict_accumulator(float)}\n",
    "     \n",
    "\n",
    "    def process(self, events ):\n",
    "\n",
    "        hists = self.hist_muon_dict.copy()\n",
    "\n",
    "        process = events.metadata[\"process\"]  # \"ttbar\" etc.\n",
    "    \n",
    "        if process != \"data\":\n",
    "            # normalization for MC\n",
    "            x_sec = events.metadata[\"xsec\"]\n",
    "            nevts_total = events.metadata[\"nevts\"]\n",
    "            print(f'events {process}: {nevts_total}')\n",
    "            #lumi = 2611.5 # /pb\n",
    "            lumi = 2256.38\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "        else:\n",
    "            xsec_weight = 1\n",
    "            print(f'events {process}')\n",
    "\n",
    "        events[\"pt_nominal\"] = 1.0\n",
    "\n",
    "        ################\n",
    "        #### Object Selection\n",
    "        ################\n",
    "        #### electrons. In this case veto_ele and selected_ele are not a subset since cutBased value is exclusive.\n",
    "        \n",
    "        #Veto electrons pT > 15 and |eta|<2.5\n",
    "        veto_ele_selection = (events.Electron.pt > 15) & (abs(events.Electron.eta) < 2.5) & (events.Electron.cutBased == 1)\n",
    "        \n",
    "        # pT > 30 GeV and |eta|<2.1 for electrons (tight electrons)\n",
    "        selected_ele_selection = (events.Electron.pt > 30) & (abs(events.Electron.eta) < 2.1) & (events.Electron.cutBased == 4)\n",
    "        \n",
    "        selected_electrons = events.Electron[ selected_ele_selection ]\n",
    "        selected_electron = (ak.count(selected_electrons.pt, axis=1) ==1 ) #we need exactly 1 electron per event fits tight characteristis\n",
    "        #print(selected_electrons) ### [ [electron1, electron2], [electron1], [], .....  ]\n",
    "        veto_electrons = events.Electron[ veto_ele_selection ]\n",
    "        veto_electron = (ak.count(veto_electrons.pt, axis=1) == 0 ) #we need exactly 0 electrons per event that fits veto characteristics\n",
    "        \n",
    "        \n",
    "        '''\n",
    "        ###comprobacion\n",
    "        if process == 'tttt':\n",
    "            N_counts = ak.num(selected_electrons.pt, axis=0)\n",
    "            print(N_counts)\n",
    "            \n",
    "            nevts_total = events.metadata[\"nevts\"]\n",
    "            \n",
    "            lumi = 2256.38\n",
    "            xsec_weight = x_sec * lumi / nevts_total\n",
    "            \n",
    "            value = N_counts * xsec_weight\n",
    "            print(value)\n",
    "            \n",
    "            value2= 821*lumi*1000\n",
    "            print(value2)\n",
    "        '''\n",
    "\n",
    "        #pT > 30 GeV & |eta| < 2.5 for jets\n",
    "        jet_selection = (events.Jet.pt * events[\"pt_nominal\"] > 30) & (abs(events.Jet.eta) < 2.5) & (events.Jet.jetId > 1)\n",
    "        selected_jets = events.Jet[jet_selection]\n",
    "        nearest_electron = selected_jets.nearest(selected_electrons, threshold=0.4)\n",
    "        # selected_jets contains both, jets and bjets\n",
    "        selected_jets = selected_jets[ ~ak.is_none(nearest_electron) ]\n",
    "        \n",
    "        ## the results of these 2 lines should be equivalent to the 2 lines above\n",
    "        #lepton_mask = ak.any(selected_jets.metric_table(selected_lepton, metric=lambda j, e: ak.local_index(j, axis=1) == e.jetIdx,), axis=2)\n",
    "        #selected_jets = selected_jets[~lepton_mask]\n",
    "        \n",
    "        B_TAG_THRESHOLD = 0.8\n",
    "        selected_bjets = events.Jet[jet_selection & ~ak.is_none(nearest_electron) & (events.Jet.btagCSVV2 >= B_TAG_THRESHOLD)]\n",
    "        selected_jets_nobjets = events.Jet[jet_selection & ~ak.is_none(nearest_electron) & ~(events.Jet.btagCSVV2 >= B_TAG_THRESHOLD)] \n",
    "        \n",
    "        ################\n",
    "        #### Event Selection\n",
    "        ################\n",
    "        \n",
    "        #Trigger\n",
    "        event_filters = ( events.HLT.Ele23_WPLoose_Gsf == 1 )\n",
    "\n",
    "        event_filters = event_filters & ( selected_electron & veto_electron )\n",
    "        \n",
    "        # at least six jets\n",
    "        event_filters = event_filters & (ak.count(selected_jets.pt, axis=1) >= 6)\n",
    "        \n",
    "        # at least two b-tagged jets (\"tag\" means score above threshold)\n",
    "        event_filters = event_filters & (ak.count(selected_bjets.pt, axis = 1) >= 2)\n",
    "        \n",
    "        # apply event filters\n",
    "        selected_events = events[event_filters]\n",
    "        selected_electrons = selected_electrons[event_filters]\n",
    "        selected_jets = selected_jets[event_filters]\n",
    "        selected_bjets = selected_bjets[event_filters]\n",
    "        \n",
    "        ###\n",
    "        ##Now let's create the variable $H_T$ (that is the sum of the p_t of all jets in each event)\n",
    "        total_pt = ak.sum(selected_jets.pt, axis=1)\n",
    "        #print(total_pt)\n",
    "        ###\n",
    "        \n",
    "        ###\n",
    "        ## Now we create the needed variables for creating H_T^{ratio} variable\n",
    "        ###\n",
    "        jets_pt = selected_jets.pt\n",
    "        ordered_jets = ak.sort(jets_pt, axis=1, ascending=False) #this array contains jets' pt ordered from the largest to the smallest value\n",
    " \n",
    "        \n",
    "        sorted_jets = []\n",
    "        sumpt_remaining_jets = []\n",
    "        if len(ordered_jets) > 0: #this if clause is to just take in account no empty arrays\n",
    "            # the array sliced_jets contains just the first four highest values of jets' pt in each event(subarray)\n",
    "            sliced_jets = ordered_jets[:,:4]\n",
    "            #print(\"Sliced Jets\")\n",
    "            #print(sliced_jets)\n",
    "            ## the array sorted_jets cotains the sum of the four highest values of jets' pt in the event\n",
    "            sorted_jets = ak.sum(sliced_jets, axis=1)\n",
    "            #print(sorted_jets)\n",
    "            #remaining_jets contains the values of the remaining pt jets excluding the four highest values\n",
    "            remaining_jets = ordered_jets[:,4:]\n",
    "            #sumpt_reamaining_jets contains the sum of the remaining pt jets excluding the four highest values\n",
    "            sumpt_remaining_jets = ak.sum(remaining_jets, axis=1)\n",
    "        \n",
    "        ##Finally we create the variable H_T^{ratio}\n",
    "        Ht_ratio = [x / y for x, y in zip(sumpt_remaining_jets,sorted_jets)]\n",
    "        #Ht_ratio2 = [x / y for x, y in zip(sorted_jets,sumpt_remaining_jets)]\n",
    "        \n",
    "        \n",
    "            \n",
    "        ###\n",
    "        ### The next lines of code are used for creating a 'super' variable i.e. a variable similar to a BDT analysis \n",
    "        ### which follows some constrains or restrictions\n",
    "        ###\n",
    "        \n",
    "        n_jets = ak.count(selected_jets.pt, axis=1) ##this is number of jets\n",
    "        #print(n_jets)\n",
    "        n_bjets = ak.count(selected_bjets.pt, axis=1) ## this is number of bjets  \n",
    "        ht_bjets = ak.sum(selected_bjets.pt, axis=1)\n",
    "                \n",
    "        for ivar in [ \"pt\", \"eta\" ]:\n",
    "            \n",
    "            hists[f'ele_{ivar}'].fill(\n",
    "                        var=ak.flatten(getattr(selected_electrons, ivar)), process=process,\n",
    "                        variation=\"nominal\", weight=xsec_weight\n",
    "                    )\n",
    "            hists[f'jets_{ivar}'].fill(\n",
    "                        var=ak.flatten(getattr(selected_jets, ivar)), process=process,\n",
    "                        variation=\"nominal\", weight=xsec_weight\n",
    "                    )\n",
    "            \n",
    "        hists['neles'].fill(\n",
    "                    var=ak.count(selected_electrons.pt, axis=1), process=process,\n",
    "                    variation=\"nominal\", weight=xsec_weight\n",
    "                )\n",
    "        hists['njets'].fill(\n",
    "                    var=n_jets, process=process,\n",
    "                    variation=\"nominal\", weight=xsec_weight\n",
    "                )\n",
    "        hists['nbjets'].fill(\n",
    "                    var=n_bjets, process=process,\n",
    "                    variation=\"nominal\", weight=xsec_weight\n",
    "                )\n",
    "        hists['sumbjets_pt'].fill(\n",
    "                    var= ht_bjets, process=process,\n",
    "                    variation=\"nominal\", weight=xsec_weight\n",
    "                ) \n",
    "        hists['H_pt'].fill(\n",
    "                    var= total_pt, process=process,\n",
    "                    variation=\"nominal\", weight=xsec_weight\n",
    "                )\n",
    "        hists['Hratio_pt'].fill(\n",
    "                    var= Ht_ratio, process=process,\n",
    "                    variation=\"nominal\", weight=xsec_weight\n",
    "                )\n",
    "            \n",
    "        if process == 'tttt':\n",
    "            self.n_jets_sig.extend(n_jets)\n",
    "            self.n_bjets_sig.extend(n_bjets)\n",
    "            self.ht_sig.extend(total_pt)\n",
    "            self.htb_sig.extend(ht_bjets)\n",
    "            self.htratio_sig.extend(Ht_ratio)\n",
    "            \n",
    "        if process != 'data' and process != 'tttt':\n",
    "            self.n_jets_bkg.extend(n_jets)\n",
    "            self.n_bjets_bkg.extend(n_bjets)\n",
    "            self.ht_bkg.extend(total_pt)\n",
    "            self.htb_bkg.extend(ht_bjets)\n",
    "            self.htratio_bkg.extend(Ht_ratio)\n",
    "                \n",
    "        if process == 'ttbar':\n",
    "            self.n_jets_tt.extend(n_jets)\n",
    "            self.n_bjets_tt.extend(n_bjets)\n",
    "            self.ht_tt.extend(total_pt)\n",
    "            self.htb_tt.extend(ht_bjets)\n",
    "            self.htratio_tt.extend(Ht_ratio)\n",
    "                \n",
    "        if process == 'data':\n",
    "            self.n_jets_data.extend(n_jets)\n",
    "            self.n_bjets_data.extend(n_bjets)\n",
    "            self.ht_data.extend(total_pt)\n",
    "            self.htb_data.extend(ht_bjets)\n",
    "            self.htratio_data.extend(Ht_ratio)\n",
    "                \n",
    "\n",
    "        output = {\"nevents\": {events.metadata[\"dataset\"]: len(events)}, \"hists\" : hists\n",
    "                    , \"n_jets_sig\": self.n_jets_sig\n",
    "                    , \"n_bjets_sig\": self.n_bjets_sig\n",
    "                    , \"ht_sig\": self.ht_sig\n",
    "                    , \"htb_sig\": self.htb_sig\n",
    "                    , \"htratio_sig\": self.htratio_sig\n",
    "                      \n",
    "                    , \"n_jets_bkg\": self.n_jets_bkg\n",
    "                    , \"n_bjets_bkg\": self.n_bjets_bkg\n",
    "                    , \"ht_bkg\": self.ht_bkg\n",
    "                    , \"htb_bkg\": self.htb_bkg                     \n",
    "                    , \"htratio_bkg\": self.htratio_bkg\n",
    "                      \n",
    "                    , \"n_jets_data\": self.n_jets_data\n",
    "                    , \"n_bjets_data\": self.n_bjets_data\n",
    "                    , \"ht_data\": self.ht_data\n",
    "                    , \"htb_data\": self.htb_data\n",
    "                    , \"htratio_data\": self.htratio_data\n",
    "                      \n",
    "                    , \"n_jets_tt\": self.n_jets_tt\n",
    "                    , \"n_bjets_tt\": self.n_bjets_tt\n",
    "                    , \"ht_tt\": self.ht_tt\n",
    "                    , \"htb_tt\": self.htb_tt\n",
    "                    , \"htratio_tt\": self.htratio_tt\n",
    "                    }\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "    def postprocess(self, accumulator):\n",
    "        \n",
    "        return accumulator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a37346f",
   "metadata": {},
   "source": [
    "Let's make it run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df430212",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a729889f9344ca3a43c21d902ee0b61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Preprocessing:   0%|          | 0/87 [00:00<?, ?file/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3d0324ede8d485cb0dc4eb522745f4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing:   0%|          | 0/112 [00:00<?, ?chunk/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events data\n",
      "events tttt: 1210521\n",
      "events tttt: 1210521\n",
      "events tttt: 1210521\n",
      "events tttt: 1210521\n",
      "events tttt: 1210521\n",
      "events tttt: 1210521\n",
      "events tttt: 1210521\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events wjets: 11817704\n",
      "events single_top_tW: 1999400\n",
      "events single_top_tW: 1999400\n",
      "events single_top_tW: 1999400\n",
      "events single_top_tW: 1999400\n",
      "events single_top_tW: 1999400\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n",
      "events ttbar: 11378043\n"
     ]
    }
   ],
   "source": [
    "#n_jets_data = []\n",
    "executor = processor.FuturesExecutor(workers=NUM_CORES)\n",
    "\n",
    "run = processor.Runner(executor=executor, schema=NanoAODSchema, \n",
    "                       savemetrics=True, metadata_cache={}, chunksize=CHUNKSIZE)\n",
    "\n",
    "t0 = time.monotonic()\n",
    "\n",
    "all_histograms, metrics = run(fileset, \"Events\", processor_instance = fourTopAnalysis(DATASET=DATA))\n",
    "exec_time = time.monotonic() - t0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "711a4872",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pickle\n",
    "\n",
    "with open(\"histograms.pkl\", \"wb\") as f:\n",
    "    pickle.dump(all_histograms[\"hists\"], f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a66cfd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Saving arrays so that we can use them without running the whole code again\n",
    "\n",
    "##signal\n",
    "n_jets_sig = np.array(all_histograms[\"n_jets_sig\"])\n",
    "n_bjets_sig = np.array(all_histograms[\"n_bjets_sig\"])\n",
    "ht_sig = np.array(all_histograms[\"ht_sig\"])\n",
    "htb_sig = np.array(all_histograms[\"htb_sig\"])\n",
    "htratio_sig = np.array(all_histograms[\"htratio_sig\"])\n",
    "\n",
    "np.save('n_jets_sig.npy', n_jets_sig )\n",
    "np.save('n_bjets_sig.npy', n_bjets_sig )\n",
    "np.save('ht_sig.npy', ht_sig )\n",
    "np.save('htb_sig.npy', htb_sig )\n",
    "np.save('htratio_sig.npy', htratio_sig )\n",
    "\n",
    "##background\n",
    "n_jets_bkg = np.array(all_histograms[\"n_jets_bkg\"])\n",
    "n_bjets_bkg = np.array(all_histograms[\"n_bjets_bkg\"])\n",
    "ht_bkg = np.array(all_histograms[\"ht_bkg\"])\n",
    "htb_bkg = np.array(all_histograms[\"htb_bkg\"])\n",
    "htratio_bkg = np.array(all_histograms[\"htratio_bkg\"])\n",
    "\n",
    "np.save('n_jets_bkg.npy', n_jets_bkg )\n",
    "np.save('n_bjets_bkg.npy', n_bjets_bkg )\n",
    "np.save('ht_bkg.npy', ht_bkg )\n",
    "np.save('htb_bkg.npy', htb_bkg )\n",
    "np.save('htratio_bkg.npy', htratio_bkg )\n",
    "\n",
    "##data\n",
    "n_jets_data = np.array(all_histograms[\"n_jets_data\"])\n",
    "n_bjets_data = np.array(all_histograms[\"n_bjets_data\"])\n",
    "ht_data = np.array(all_histograms[\"ht_data\"])\n",
    "htb_data = np.array(all_histograms[\"htb_data\"])\n",
    "htratio_data = np.array(all_histograms[\"htratio_data\"])\n",
    "\n",
    "np.save('n_jets_data.npy', n_jets_data )\n",
    "np.save('n_bjets_data.npy', n_bjets_data )\n",
    "np.save('ht_data.npy', ht_data )\n",
    "np.save('htb_data.npy', htb_data )\n",
    "np.save('htratio_data.npy', htratio_data )\n",
    "\n",
    "##tt\n",
    "n_jets_tt = np.array(all_histograms[\"n_jets_tt\"])\n",
    "n_bjets_tt = np.array(all_histograms[\"n_bjets_tt\"])\n",
    "ht_tt = np.array(all_histograms[\"ht_tt\"])\n",
    "htb_tt = np.array(all_histograms[\"htb_tt\"])\n",
    "htratio_tt = np.array(all_histograms[\"htratio_tt\"])\n",
    "\n",
    "np.save('n_jets_tt.npy', n_jets_tt )\n",
    "np.save('n_bjets_tt.npy', n_bjets_tt )\n",
    "np.save('ht_tt.npy', ht_tt )\n",
    "np.save('htb_tt.npy', htb_tt )\n",
    "np.save('htratio_tt.npy', htratio_tt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13d6e55f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "event rate per worker (full execution time divided by NUM_CORES=4): 9.53 kHz\n",
      "event rate per worker (pure processtime): 11.79 kHz\n",
      "amount of data read: 2393.35 MB\n"
     ]
    }
   ],
   "source": [
    "dataset_source = \"/data\" if fileset[\"ttbar__nominal\"][\"files\"][0].startswith(\"/data\") else \"https://xrootd-local.unl.edu:1094\" # TODO: xcache support\n",
    "metrics.update({\"walltime\": exec_time, \"num_workers\": NUM_CORES, \"dataset_source\": dataset_source, \n",
    "                \"n_files_max_per_sample\": N_FILES_MAX_PER_SAMPLE, \n",
    "                \"cores_per_worker\": CORES_PER_WORKER, \"chunksize\": CHUNKSIZE})#\n",
    "\n",
    "print(f\"event rate per worker (full execution time divided by NUM_CORES={NUM_CORES}): {metrics['entries'] / NUM_CORES / exec_time / 1_000:.2f} kHz\")\n",
    "print(f\"event rate per worker (pure processtime): {metrics['entries'] / metrics['processtime'] / 1_000:.2f} kHz\")\n",
    "print(f\"amount of data read: {metrics['bytesread']/1000**2:.2f} MB\")  # likely buggy: https://github.com/CoffeaTeam/coffea/issues/717\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0178c8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
